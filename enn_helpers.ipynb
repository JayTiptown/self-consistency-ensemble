{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_xet in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (1.1.3)\n",
      "Requirement already satisfied: torch in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.22.0)\n",
      "Requirement already satisfied: transformers in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (4.51.3)\n",
      "Requirement already satisfied: tqdm in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: requests in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: datasets in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (3.5.1)\n",
      "Requirement already satisfied: accelerate in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (1.6.0)\n",
      "Requirement already satisfied: bitsandbytes in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.42.0)\n",
      "Requirement already satisfied: tensorboard in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: torch-tb-profiler in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.4.3)\n",
      "Requirement already satisfied: openai in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (1.70.0)\n",
      "Requirement already satisfied: anthropic in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.50.0)\n",
      "Requirement already satisfied: google-generativeai in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.8.5)\n",
      "Requirement already satisfied: filelock in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torchvision) (2.2.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: psutil in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: scipy in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from bitsandbytes) (1.15.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tensorboard) (3.8)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tensorboard) (5.29.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tensorboard) (65.5.0)\n",
      "Requirement already satisfied: six>1.9 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai) (2.11.1)\n",
      "Requirement already satisfied: sniffio in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from google-generativeai) (2.25.0rc1)\n",
      "Requirement already satisfied: google-api-python-client in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from google-generativeai) (2.169.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from google-generativeai) (2.40.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install hf_xet torch torchvision transformers tqdm requests datasets accelerate bitsandbytes tensorboard torch-tb-profiler openai anthropic google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Safe flags for Apple-silicon ---\n",
    "import os, platform\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]           = \"false\"   # avoid fork-after-tokenizer bug\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.8\"     # leave 10 % headroom, prevents sudden kills\n",
    "os.environ[\"PYTORCH_MPS_LOW_WATERMARK_RATIO\"] = \"0.5\"\n",
    "os.environ[\"FLASH_ATTENTION_FORCE_DISABLE\"]    = \"1\"       # disable Flash-Attn v2 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import platform\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, logging\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb_writer = SummaryWriter(\"runs/halueval_llama\")\n",
    "\n",
    "# Reduce verbosity of transformers\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\") # change this to foundry gpu if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.7.0   Free RAM: 4.542480384 GB\n",
      "MPS cap: True\n"
     ]
    }
   ],
   "source": [
    "import torch, os, platform, psutil, time\n",
    "print(\"Torch:\", torch.__version__, \"  Free RAM:\", psutil.virtual_memory().available/1e9, \"GB\")\n",
    "print(\"MPS cap:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macOS-13.4-arm64-arm-64bit 2.7.0\n",
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "print(platform.platform(), torch.__version__)\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base model using Llama from Hugging Face\n",
    "class LlamaBaseNet(nn.Module):\n",
    "    def __init__(self, model_name, num_classes=2):\n",
    "        super().__init__()\n",
    "        # Load Llama model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        # self.backbone = self.backbone.half()           # fp16\n",
    "        self.backbone.gradient_checkpointing_enable()  # save RAM\n",
    "        \n",
    "        # If the tokenizer doesn't have a padding token, set it\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        # Get hidden size from config\n",
    "        self.hidden_size = self.backbone.config.hidden_size\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(self.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, texts):\n",
    "        # Tokenize and move to device\n",
    "        if isinstance(texts, torch.Tensor):\n",
    "            # If input is already tokenized\n",
    "            inputs = {'input_ids': texts}\n",
    "        else:\n",
    "            # If input is raw text\n",
    "            inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        inputs = {k: v.to(self.classifier.weight.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get model outputs\n",
    "        with torch.no_grad():  # Don't compute gradients for the backbone\n",
    "            outputs = self.backbone(**inputs)\n",
    "            \n",
    "        # Use the last hidden state of the last token for classification\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        sequence_lengths = torch.ne(inputs['input_ids'], self.tokenizer.pad_token_id).sum(-1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        \n",
    "        # Get the hidden state for the last token in each sequence\n",
    "        features = last_hidden_states[torch.arange(batch_size), sequence_lengths]\n",
    "        \n",
    "        # Apply classifier\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hugging face auth\n",
    "\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGING_FACE_KEY\")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HaluEval dataset from Hugging Face...\n",
      "Loading qa dataset...\n",
      "Loading dialogue dataset...\n",
      "Loading summarization dataset...\n",
      "Loading general dataset...\n",
      "Merging all training data...\n",
      "Merging all test data...\n",
      "HaluEval dataset preparation complete!\n",
      "Train data: data/halueval/train.jsonl\n",
      "Test data: data/halueval/test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Load HaluEval dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "def prepare_halueval_data_from_hf():\n",
    "    \"\"\"Load HaluEval dataset from Hugging Face\"\"\"\n",
    "    print(\"Loading HaluEval dataset from Hugging Face...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = \"data/halueval\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each split\n",
    "    categories = [\"qa\", \"dialogue\", \"summarization\", \"general\"]\n",
    "    \n",
    "    # Prepare train and test sets\n",
    "    for category in categories:\n",
    "        print(f\"Loading {category} dataset...\")\n",
    "        # Load the dataset for this category\n",
    "        dataset = load_dataset(\"pminervini/HaluEval\", category)\n",
    "        \n",
    "        # The dataset has a 'data' split containing all examples\n",
    "        data = dataset['data']\n",
    "        \n",
    "        # Split into train/test (80/20 split)\n",
    "        splits = data.train_test_split(test_size=0.2, seed=42)\n",
    "        \n",
    "        # Save as jsonl\n",
    "        with open(f\"{output_dir}/{category}_train.jsonl\", 'w', encoding='utf-8') as f:\n",
    "            for item in splits['train']:\n",
    "                formatted_item = {\n",
    "                    'question': item.get('instruction', ''),\n",
    "                    'response': item.get('output', ''),\n",
    "                    'is_hallucination': 1 if item.get('label') == 'hallucinated' else 0\n",
    "                }\n",
    "                f.write(json.dumps(formatted_item) + '\\n')\n",
    "        \n",
    "        with open(f\"{output_dir}/{category}_test.jsonl\", 'w', encoding='utf-8') as f:\n",
    "            for item in splits['test']:\n",
    "                formatted_item = {\n",
    "                    'question': item.get('instruction', ''),\n",
    "                    'response': item.get('output', ''),\n",
    "                    'is_hallucination': 1 if item.get('label') == 'hallucinated' else 0\n",
    "                }\n",
    "                f.write(json.dumps(formatted_item) + '\\n')\n",
    "    \n",
    "    # Merge all training data\n",
    "    print(\"Merging all training data...\")\n",
    "    with open(f\"{output_dir}/train.jsonl\", 'w', encoding='utf-8') as outfile:\n",
    "        for category in categories:\n",
    "            with open(f\"{output_dir}/{category}_train.jsonl\", 'r', encoding='utf-8') as infile:\n",
    "                outfile.write(infile.read())\n",
    "    \n",
    "    # Merge all test data\n",
    "    print(\"Merging all test data...\")\n",
    "    with open(f\"{output_dir}/test.jsonl\", 'w', encoding='utf-8') as outfile:\n",
    "        for category in categories:\n",
    "            with open(f\"{output_dir}/{category}_test.jsonl\", 'r', encoding='utf-8') as infile:\n",
    "                outfile.write(infile.read())\n",
    "    \n",
    "    print(\"HaluEval dataset preparation complete!\")\n",
    "    print(f\"Train data: {output_dir}/train.jsonl\")\n",
    "    print(f\"Test data: {output_dir}/test.jsonl\")\n",
    "    \n",
    "    return f\"{output_dir}/train.jsonl\", f\"{output_dir}/test.jsonl\"\n",
    "\n",
    "# Run the function to get the paths\n",
    "train_data_path, test_data_path = prepare_halueval_data_from_hf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define the Epinet\n",
    "class EpiNet(nn.Module):\n",
    "    def __init__(self, feature_dim, z_dim, hidden_dims, num_classes):\n",
    "        super().__init__()\n",
    "        dims = [feature_dim + z_dim] + hidden_dims + [num_classes]\n",
    "        layers = []\n",
    "        for in_d, out_d in zip(dims, dims[1:]):\n",
    "            layers += [nn.Linear(in_d, out_d), nn.ReLU()]\n",
    "        self.mlp = nn.Sequential(*layers[:-1])  # drop final ReLU\n",
    "\n",
    "    def forward(self, features, z):\n",
    "        # stop-gradient on features\n",
    "        features = features.detach()\n",
    "        x = torch.cat([features, z], dim=1)\n",
    "        return self.mlp(x)\n",
    "\n",
    "# 4. Define the PriorNet\n",
    "class PriorNet(nn.Module):\n",
    "    def __init__(self, feature_dim, z_dim, num_classes):\n",
    "        super().__init__()\n",
    "        # Fixed random weights\n",
    "        self.fc = nn.Linear(feature_dim + z_dim, num_classes)\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False  # fix weights\n",
    "\n",
    "    def forward(self, features, z):\n",
    "        features = features.detach()\n",
    "        x = torch.cat([features, z], dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Wrap into an Epistemic Neural Network\n",
    "class EpistemicNN(nn.Module):\n",
    "    def __init__(self, base: LlamaBaseNet, epinet: EpiNet, prior: PriorNet=None):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.epinet = epinet\n",
    "        self.prior = prior\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        logits, features = self.base(x)         # base logits & features\n",
    "        δ = self.epinet(features, z)            # learnable correction\n",
    "        σP = self.prior(features, z) if self.prior else 0\n",
    "        return logits + δ + σP\n",
    "\n",
    "# 6. Sampling epistemic index z\n",
    "def sample_z(batch_size, z_dim, device):\n",
    "    # Gaussian prior\n",
    "    return torch.randn(batch_size, z_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_enn(model, dataloader, epochs, lr, λ, z_dim, device, writer):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=λ)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(dataloader):\n",
    "            # Move tensors to device\n",
    "            x_batch = x_batch.to(device, non_blocking=False)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            # Sample epistemic indices\n",
    "            z = sample_z(len(y_batch), z_dim, device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(x_batch, z)\n",
    "            loss = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track accuracy\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += predicted.eq(y_batch).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}, \"\n",
    "                      f\"Acc: {100.*correct/total:.2f}%\")\n",
    "                \n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            acc      = 100. * correct / total\n",
    "            global_step = epoch * len(dataloader) + batch_idx\n",
    "            writer.add_scalar(\"train/loss\", loss.item(), global_step)\n",
    "            writer.add_scalar(\"train/acc\",  acc,      global_step)\n",
    "\n",
    "            if global_step % 50 == 0:\n",
    "                writer.flush()\n",
    "                \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {total_loss/(batch_idx+1):.4f}, \"\n",
    "              f\"Accuracy: {100.*correct/total:.2f}%\")\n",
    "\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Also update the evaluation function\n",
    "def evaluate_enn(model, dataloader, z_dim, device, num_samples=10):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epistemic_uncertainty = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch = x_batch.to(device, non_blocking=False)\n",
    "            y_batch = y_batch.to(device)\n",
    "            batch_size = len(x_batch)\n",
    "            \n",
    "            # Sample multiple z for each input\n",
    "            all_logits = []\n",
    "            for _ in range(num_samples):\n",
    "                z = sample_z(batch_size, z_dim, device)\n",
    "                logits = model(x_batch, z)\n",
    "                all_logits.append(logits)\n",
    "            \n",
    "            # Stack all predictions\n",
    "            stacked_logits = torch.stack(all_logits)  # [num_samples, batch_size, num_classes]\n",
    "            \n",
    "            # Mean prediction\n",
    "            mean_logits = stacked_logits.mean(dim=0)\n",
    "            _, predicted = mean_logits.max(1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += predicted.eq(y_batch).sum().item()\n",
    "            \n",
    "            # Calculate uncertainty - variance across samples\n",
    "            uncertainty = stacked_logits.var(dim=0).sum(dim=1)  # [batch_size]\n",
    "            epistemic_uncertainty.append(uncertainty)\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    avg_uncertainty = torch.cat(epistemic_uncertainty).mean().item()\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Average Epistemic Uncertainty: {avg_uncertainty:.4f}\")\n",
    "    \n",
    "    return accuracy, avg_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
