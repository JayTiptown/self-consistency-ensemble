{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.22.0)\n",
      "Requirement already satisfied: transformers in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (4.51.3)\n",
      "Requirement already satisfied: tqdm in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: requests in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: datasets in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (3.5.1)\n",
      "Requirement already satisfied: accelerate in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (1.6.0)\n",
      "Requirement already satisfied: bitsandbytes in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.42.0)\n",
      "Requirement already satisfied: tensorboard in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: torch-tb-profiler in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torchvision) (2.2.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: psutil in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: scipy in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from bitsandbytes) (1.15.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tensorboard) (3.8)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tensorboard) (6.30.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tensorboard) (65.5.0)\n",
      "Requirement already satisfied: six>1.9 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision transformers tqdm requests datasets accelerate bitsandbytes tensorboard torch-tb-profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Safe flags for Apple-silicon ---\n",
    "import os, platform\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]           = \"false\"   # avoid fork-after-tokenizer bug\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.9\"     # leave 10 % headroom, prevents sudden kills\n",
    "os.environ[\"FLASH_ATTENTION_FORCE_DISABLE\"]    = \"1\"       # disable Flash-Attn v2 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import platform\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, logging\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb_writer = SummaryWriter(\"runs/halueval_llama\")\n",
    "\n",
    "# Reduce verbosity of transformers\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\") # change this to foundry gpu if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.7.0   Free RAM: 2.966667264 GB\n",
      "MPS cap: True\n"
     ]
    }
   ],
   "source": [
    "import torch, os, platform, psutil, time\n",
    "print(\"Torch:\", torch.__version__, \"  Free RAM:\", psutil.virtual_memory().available/1e9, \"GB\")\n",
    "print(\"MPS cap:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macOS-13.4-arm64-arm-64bit 2.7.0\n",
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "print(platform.platform(), torch.__version__)\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base model using Llama from Hugging Face\n",
    "class LlamaBaseNet(nn.Module):\n",
    "    def __init__(self, model_name=\"meta-llama/Llama-2-7b-hf\", num_classes=2):\n",
    "        super().__init__()\n",
    "        # Load Llama model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        # self.backbone = self.backbone.half()           # fp16\n",
    "        self.backbone.gradient_checkpointing_enable()  # save RAM\n",
    "        \n",
    "        # If the tokenizer doesn't have a padding token, set it\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        # Get hidden size from config\n",
    "        self.hidden_size = self.backbone.config.hidden_size\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(self.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, texts):\n",
    "        # Tokenize and move to device\n",
    "        if isinstance(texts, torch.Tensor):\n",
    "            # If input is already tokenized\n",
    "            inputs = {'input_ids': texts}\n",
    "        else:\n",
    "            # If input is raw text\n",
    "            inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        inputs = {k: v.to(self.classifier.weight.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get model outputs\n",
    "        with torch.no_grad():  # Don't compute gradients for the backbone\n",
    "            outputs = self.backbone(**inputs)\n",
    "            \n",
    "        # Use the last hidden state of the last token for classification\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        sequence_lengths = torch.ne(inputs['input_ids'], self.tokenizer.pad_token_id).sum(-1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        \n",
    "        # Get the hidden state for the last token in each sequence\n",
    "        features = last_hidden_states[torch.arange(batch_size), sequence_lengths]\n",
    "        \n",
    "        # Apply classifier\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hugging face auth\n",
    "\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGING_FACE_KEY\")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HaluEval dataset from Hugging Face...\n",
      "Loading qa dataset...\n",
      "Loading dialogue dataset...\n",
      "Loading summarization dataset...\n",
      "Loading general dataset...\n",
      "Merging all training data...\n",
      "Merging all test data...\n",
      "HaluEval dataset preparation complete!\n",
      "Train data: data/halueval/train.jsonl\n",
      "Test data: data/halueval/test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Load HaluEval dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "def prepare_halueval_data_from_hf():\n",
    "    \"\"\"Load HaluEval dataset from Hugging Face\"\"\"\n",
    "    print(\"Loading HaluEval dataset from Hugging Face...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = \"data/halueval\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each split\n",
    "    categories = [\"qa\", \"dialogue\", \"summarization\", \"general\"]\n",
    "    \n",
    "    # Prepare train and test sets\n",
    "    for category in categories:\n",
    "        print(f\"Loading {category} dataset...\")\n",
    "        # Load the dataset for this category\n",
    "        dataset = load_dataset(\"pminervini/HaluEval\", category)\n",
    "        \n",
    "        # The dataset has a 'data' split containing all examples\n",
    "        data = dataset['data']\n",
    "        \n",
    "        # Split into train/test (80/20 split)\n",
    "        splits = data.train_test_split(test_size=0.2, seed=42)\n",
    "        \n",
    "        # Save as jsonl\n",
    "        with open(f\"{output_dir}/{category}_train.jsonl\", 'w', encoding='utf-8') as f:\n",
    "            for item in splits['train']:\n",
    "                formatted_item = {\n",
    "                    'question': item.get('instruction', ''),\n",
    "                    'response': item.get('output', ''),\n",
    "                    'is_hallucination': 1 if item.get('label') == 'hallucinated' else 0\n",
    "                }\n",
    "                f.write(json.dumps(formatted_item) + '\\n')\n",
    "        \n",
    "        with open(f\"{output_dir}/{category}_test.jsonl\", 'w', encoding='utf-8') as f:\n",
    "            for item in splits['test']:\n",
    "                formatted_item = {\n",
    "                    'question': item.get('instruction', ''),\n",
    "                    'response': item.get('output', ''),\n",
    "                    'is_hallucination': 1 if item.get('label') == 'hallucinated' else 0\n",
    "                }\n",
    "                f.write(json.dumps(formatted_item) + '\\n')\n",
    "    \n",
    "    # Merge all training data\n",
    "    print(\"Merging all training data...\")\n",
    "    with open(f\"{output_dir}/train.jsonl\", 'w', encoding='utf-8') as outfile:\n",
    "        for category in categories:\n",
    "            with open(f\"{output_dir}/{category}_train.jsonl\", 'r', encoding='utf-8') as infile:\n",
    "                outfile.write(infile.read())\n",
    "    \n",
    "    # Merge all test data\n",
    "    print(\"Merging all test data...\")\n",
    "    with open(f\"{output_dir}/test.jsonl\", 'w', encoding='utf-8') as outfile:\n",
    "        for category in categories:\n",
    "            with open(f\"{output_dir}/{category}_test.jsonl\", 'r', encoding='utf-8') as infile:\n",
    "                outfile.write(infile.read())\n",
    "    \n",
    "    print(\"HaluEval dataset preparation complete!\")\n",
    "    print(f\"Train data: {output_dir}/train.jsonl\")\n",
    "    print(f\"Test data: {output_dir}/test.jsonl\")\n",
    "    \n",
    "    return f\"{output_dir}/train.jsonl\", f\"{output_dir}/test.jsonl\"\n",
    "\n",
    "# Run the function to get the paths\n",
    "train_data_path, test_data_path = prepare_halueval_data_from_hf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define the Epinet\n",
    "class EpiNet(nn.Module):\n",
    "    def __init__(self, feature_dim, z_dim, hidden_dims, num_classes):\n",
    "        super().__init__()\n",
    "        dims = [feature_dim + z_dim] + hidden_dims + [num_classes]\n",
    "        layers = []\n",
    "        for in_d, out_d in zip(dims, dims[1:]):\n",
    "            layers += [nn.Linear(in_d, out_d), nn.ReLU()]\n",
    "        self.mlp = nn.Sequential(*layers[:-1])  # drop final ReLU\n",
    "\n",
    "    def forward(self, features, z):\n",
    "        # stop-gradient on features\n",
    "        features = features.detach()\n",
    "        x = torch.cat([features, z], dim=1)\n",
    "        return self.mlp(x)\n",
    "\n",
    "# 4. Define the PriorNet\n",
    "class PriorNet(nn.Module):\n",
    "    def __init__(self, feature_dim, z_dim, num_classes):\n",
    "        super().__init__()\n",
    "        # Fixed random weights\n",
    "        self.fc = nn.Linear(feature_dim + z_dim, num_classes)\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False  # fix weights\n",
    "\n",
    "    def forward(self, features, z):\n",
    "        features = features.detach()\n",
    "        x = torch.cat([features, z], dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Wrap into an Epistemic Neural Network\n",
    "class EpistemicNN(nn.Module):\n",
    "    def __init__(self, base: LlamaBaseNet, epinet: EpiNet, prior: PriorNet=None):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.epinet = epinet\n",
    "        self.prior = prior\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        logits, features = self.base(x)         # base logits & features\n",
    "        δ = self.epinet(features, z)            # learnable correction\n",
    "        σP = self.prior(features, z) if self.prior else 0\n",
    "        return logits + δ + σP\n",
    "\n",
    "# 6. Sampling epistemic index z\n",
    "def sample_z(batch_size, z_dim, device):\n",
    "    # Gaussian prior\n",
    "    return torch.randn(batch_size, z_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_enn(model, dataloader, epochs, lr, λ, z_dim, device, writer):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=λ)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(dataloader):\n",
    "            # Move tensors to device\n",
    "            x_batch = x_batch.to(device, non_blocking=False)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            # Sample epistemic indices\n",
    "            z = sample_z(len(y_batch), z_dim, device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(x_batch, z)\n",
    "            loss = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track accuracy\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += predicted.eq(y_batch).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}, \"\n",
    "                      f\"Acc: {100.*correct/total:.2f}%\")\n",
    "                \n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            acc      = 100. * correct / total\n",
    "            global_step = epoch * len(dataloader) + batch_idx\n",
    "            writer.add_scalar(\"train/loss\", loss.item(), global_step)\n",
    "            writer.add_scalar(\"train/acc\",  acc,      global_step)\n",
    "\n",
    "            if global_step % 50 == 0:\n",
    "                writer.flush()\n",
    "                \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {total_loss/(batch_idx+1):.4f}, \"\n",
    "              f\"Accuracy: {100.*correct/total:.2f}%\")\n",
    "\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Also update the evaluation function\n",
    "def evaluate_enn(model, dataloader, z_dim, device, num_samples=10):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epistemic_uncertainty = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch = x_batch.to(device, non_blocking=False)\n",
    "            y_batch = y_batch.to(device)\n",
    "            batch_size = len(x_batch)\n",
    "            \n",
    "            # Sample multiple z for each input\n",
    "            all_logits = []\n",
    "            for _ in range(num_samples):\n",
    "                z = sample_z(batch_size, z_dim, device)\n",
    "                logits = model(x_batch, z)\n",
    "                all_logits.append(logits)\n",
    "            \n",
    "            # Stack all predictions\n",
    "            stacked_logits = torch.stack(all_logits)  # [num_samples, batch_size, num_classes]\n",
    "            \n",
    "            # Mean prediction\n",
    "            mean_logits = stacked_logits.mean(dim=0)\n",
    "            _, predicted = mean_logits.max(1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += predicted.eq(y_batch).sum().item()\n",
    "            \n",
    "            # Calculate uncertainty - variance across samples\n",
    "            uncertainty = stacked_logits.var(dim=0).sum(dim=1)  # [batch_size]\n",
    "            epistemic_uncertainty.append(uncertainty)\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    avg_uncertainty = torch.cat(epistemic_uncertainty).mean().item()\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Average Epistemic Uncertainty: {avg_uncertainty:.4f}\")\n",
    "    \n",
    "    return accuracy, avg_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n",
      "Loading dataset...\n",
      "Loading qa dataset...\n",
      "Loading dialogue dataset...\n",
      "Loading summarization dataset...\n",
      "Loading general dataset...\n",
      "Training samples: 27605\n",
      "Testing samples: 6902\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_classes = 2  # Binary classification for hallucination detection\n",
    "z_dim = 16\n",
    "hidden_dims = [128, 64]\n",
    "lr = 1e-5  # Lower learning rate for LLM fine-tuning\n",
    "epochs = 3  # Reduce epochs for faster training with large model\n",
    "λ = 1e-5\n",
    "batch_size = 1  # Smaller batch size for large model\n",
    "max_length = 128\n",
    "import random\n",
    "\n",
    "# Choose a Llama model - use a smaller version if memory is limited\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Start with smaller model for testing\n",
    "\n",
    "class HaluEvalDataset(Dataset):\n",
    "    def __init__(self, data_items, tokenizer):\n",
    "        self.data = data_items\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        # ---------- 1. pick the *content* string ----------\n",
    "        if \"question\" in item:          # QA\n",
    "            content = f\"Context: {item['knowledge']}\\nQuestion: {item['question']}\"\n",
    "            gold   = item[\"right_answer\"]\n",
    "            halluc = item[\"hallucinated_answer\"]\n",
    "\n",
    "        elif \"dialogue_history\" in item:      # Dialogue\n",
    "            content = f\"Context: {item['knowledge']}\\nDialogue: {item['dialogue_history']}\"\n",
    "            gold   = item[\"right_response\"]\n",
    "            halluc = item[\"hallucinated_response\"]\n",
    "\n",
    "        elif \"document\" in item:              # Summarisation\n",
    "            content = f\"Document: {item['document']}\"\n",
    "            gold   = item[\"right_summary\"]\n",
    "            halluc = item[\"hallucinated_summary\"]\n",
    "\n",
    "        else:                                 # General split\n",
    "            content = f\"User query: {item['user_query']}\"\n",
    "            # general‐split has only one answer plus a Yes/No label\n",
    "            gold   = item[\"chatgpt_response\"]\n",
    "            halluc = None                     # there is no alt-answer\n",
    "\n",
    "        # ---------- 2. choose gold vs hallucinated version ----------\n",
    "        if halluc is not None and torch.rand(1).item() > 0.5:\n",
    "            text  = f\"{content}\\nAnswer: {halluc}\"\n",
    "            label = 1            # hallucination\n",
    "        else:\n",
    "            text  = f\"{content}\\nAnswer: {gold}\"\n",
    "            label = 0            # factual\n",
    "\n",
    "        # ---------- 3. tokenise ----------\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return enc[\"input_ids\"].squeeze(0), torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "# Initialize models\n",
    "print(\"Initializing models...\")\n",
    "base = LlamaBaseNet(model_name, num_classes).to(device)\n",
    "feature_dim = base.hidden_size\n",
    "epinet = EpiNet(feature_dim, z_dim, hidden_dims, num_classes).to(device)\n",
    "prior = PriorNet(feature_dim, z_dim, num_classes).to(device)\n",
    "enn = EpistemicNN(base, epinet, prior).to(device)\n",
    "\n",
    "# Create train/test splits from the loaded HaluEval dataset\n",
    "print(\"Loading dataset...\")\n",
    "all_data = []\n",
    "categories = [\"qa\", \"dialogue\", \"summarization\", \"general\"]\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"Loading {category} dataset...\")\n",
    "    dataset = load_dataset(\"pminervini/HaluEval\", category)\n",
    "    all_data.extend(dataset['data'])\n",
    "\n",
    "# Shuffle and split the data\n",
    "random.seed(42)\n",
    "random.shuffle(all_data)\n",
    "split_idx = int(0.8 * len(all_data))\n",
    "train_data = all_data[:split_idx]\n",
    "test_data = all_data[split_idx:]\n",
    "\n",
    "# -------------------- build proper Dataset objects --------------------\n",
    "train_dataset = HaluEvalDataset(train_data, base.tokenizer)\n",
    "test_dataset  = HaluEvalDataset(test_data,  base.tokenizer)\n",
    "\n",
    "trainloader = DataLoader(train_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "\n",
    "testloader  = DataLoader(test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Testing samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "# enn.eval();                # no grads\n",
    "# x,y = next(iter(small_loader))\n",
    "# z   = sample_z(1, z_dim, device)\n",
    "# with torch.no_grad():      # forward only\n",
    "#     _ = enn(x.to(device), z)\n",
    "# print(\"✓ tiny batch ran\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1402"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, gc\n",
    "\n",
    "torch.mps.empty_cache()   # releases unused cached blocks\n",
    "gc.collect()              # Python-side garbage collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 95975), started 0:00:15 ago. (Use '!kill 95975' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7e923c575d0b7acc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7e923c575d0b7acc\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaytipirneni/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Batch 0, Loss: 4.2329, Acc: 0.00%\n",
      "Epoch 1/3, Batch 10, Loss: 1.6306, Acc: 36.36%\n",
      "Epoch 1/3, Batch 20, Loss: 0.1659, Acc: 57.14%\n",
      "Epoch 1/3, Batch 30, Loss: 1.9210, Acc: 51.61%\n",
      "Epoch 1/3, Batch 40, Loss: 0.4872, Acc: 56.10%\n",
      "Epoch 1/3, Batch 50, Loss: 1.0253, Acc: 52.94%\n",
      "Epoch 1/3, Batch 60, Loss: 2.2024, Acc: 54.10%\n",
      "Epoch 1/3, Batch 70, Loss: 3.1466, Acc: 56.34%\n",
      "Epoch 1/3, Batch 80, Loss: 1.1493, Acc: 50.62%\n",
      "Epoch 1/3, Batch 90, Loss: 0.5534, Acc: 52.75%\n",
      "Epoch 1/3, Batch 100, Loss: 0.0941, Acc: 51.49%\n",
      "Epoch 1/3, Batch 110, Loss: 0.0243, Acc: 53.15%\n",
      "Epoch 1/3, Batch 120, Loss: 0.6817, Acc: 54.55%\n",
      "Epoch 1/3, Batch 130, Loss: 0.0532, Acc: 53.44%\n",
      "Epoch 1/3, Batch 140, Loss: 2.5350, Acc: 53.19%\n",
      "Epoch 1/3, Batch 150, Loss: 0.9787, Acc: 54.30%\n",
      "Epoch 1/3, Batch 160, Loss: 3.2568, Acc: 54.66%\n",
      "Epoch 1/3, Batch 170, Loss: 0.0961, Acc: 55.56%\n",
      "Epoch 1/3, Batch 180, Loss: 0.0854, Acc: 56.35%\n",
      "Epoch 1/3, Batch 190, Loss: 0.1460, Acc: 56.54%\n",
      "Epoch 1/3, Batch 200, Loss: 0.3086, Acc: 55.72%\n",
      "Epoch 1/3, Batch 210, Loss: 0.0716, Acc: 56.87%\n",
      "Epoch 1/3, Batch 220, Loss: 0.1050, Acc: 57.01%\n",
      "Epoch 1/3, Batch 230, Loss: 1.1849, Acc: 56.71%\n",
      "Epoch 1/3, Batch 240, Loss: 2.4424, Acc: 56.43%\n",
      "Epoch 1/3, Batch 250, Loss: 1.3400, Acc: 56.57%\n",
      "Epoch 1/3, Batch 260, Loss: 0.0562, Acc: 56.32%\n",
      "Epoch 1/3, Batch 270, Loss: 0.4576, Acc: 57.20%\n",
      "Epoch 1/3, Batch 280, Loss: 0.3787, Acc: 56.58%\n",
      "Epoch 1/3, Batch 290, Loss: 0.9098, Acc: 56.36%\n",
      "Epoch 1/3, Batch 300, Loss: 0.3648, Acc: 56.15%\n",
      "Epoch 1/3, Batch 310, Loss: 0.4454, Acc: 55.95%\n",
      "Epoch 1/3, Batch 320, Loss: 0.8375, Acc: 55.45%\n",
      "Epoch 1/3, Batch 330, Loss: 0.7968, Acc: 54.68%\n",
      "Epoch 1/3, Batch 340, Loss: 0.3979, Acc: 55.13%\n",
      "Epoch 1/3, Batch 350, Loss: 0.1023, Acc: 55.27%\n",
      "Epoch 1/3, Batch 360, Loss: 1.6019, Acc: 55.68%\n",
      "Epoch 1/3, Batch 370, Loss: 2.3796, Acc: 55.53%\n",
      "Epoch 1/3, Batch 380, Loss: 0.0184, Acc: 55.91%\n",
      "Epoch 1/3, Batch 390, Loss: 0.0120, Acc: 56.27%\n",
      "Epoch 1/3, Batch 400, Loss: 1.6208, Acc: 55.86%\n",
      "Epoch 1/3, Batch 410, Loss: 0.0421, Acc: 55.72%\n",
      "Epoch 1/3, Batch 420, Loss: 1.5743, Acc: 55.82%\n",
      "Epoch 1/3, Batch 430, Loss: 0.7099, Acc: 55.92%\n",
      "Epoch 1/3, Batch 440, Loss: 0.0838, Acc: 55.78%\n",
      "Epoch 1/3, Batch 450, Loss: 3.7260, Acc: 55.43%\n",
      "Epoch 1/3, Batch 460, Loss: 0.0194, Acc: 54.88%\n",
      "Epoch 1/3, Batch 470, Loss: 0.2930, Acc: 54.56%\n",
      "Epoch 1/3, Batch 480, Loss: 0.6754, Acc: 53.85%\n",
      "Epoch 1/3, Batch 490, Loss: 3.0525, Acc: 53.77%\n",
      "Epoch 1/3, Batch 500, Loss: 2.0332, Acc: 53.69%\n",
      "Epoch 1/3, Batch 510, Loss: 0.0645, Acc: 53.42%\n",
      "Epoch 1/3, Batch 520, Loss: 0.1013, Acc: 53.36%\n",
      "Epoch 1/3, Batch 530, Loss: 1.8825, Acc: 53.11%\n",
      "Epoch 1/3, Batch 540, Loss: 3.9080, Acc: 52.68%\n",
      "Epoch 1/3, Batch 550, Loss: 0.7017, Acc: 52.81%\n",
      "Epoch 1/3, Batch 560, Loss: 1.1351, Acc: 53.12%\n",
      "Epoch 1/3, Batch 570, Loss: 2.1299, Acc: 53.06%\n",
      "Epoch 1/3, Batch 580, Loss: 0.0265, Acc: 53.18%\n",
      "Epoch 1/3, Batch 590, Loss: 0.3329, Acc: 52.96%\n",
      "Epoch 1/3, Batch 600, Loss: 1.0747, Acc: 52.91%\n",
      "Epoch 1/3, Batch 610, Loss: 1.0940, Acc: 53.03%\n",
      "Epoch 1/3, Batch 620, Loss: 0.4364, Acc: 53.62%\n",
      "Epoch 1/3, Batch 630, Loss: 0.2221, Acc: 53.72%\n",
      "Epoch 1/3, Batch 640, Loss: 0.5461, Acc: 53.67%\n",
      "Epoch 1/3, Batch 650, Loss: 1.8121, Acc: 53.76%\n",
      "Epoch 1/3, Batch 660, Loss: 0.0161, Acc: 53.86%\n",
      "Epoch 1/3, Batch 670, Loss: 0.2648, Acc: 53.80%\n",
      "Epoch 1/3, Batch 680, Loss: 0.9702, Acc: 53.74%\n",
      "Epoch 1/3, Batch 690, Loss: 0.8596, Acc: 53.55%\n",
      "Epoch 1/3, Batch 700, Loss: 0.2916, Acc: 53.78%\n",
      "Epoch 1/3, Batch 710, Loss: 0.2020, Acc: 53.45%\n",
      "Epoch 1/3, Batch 720, Loss: 1.4691, Acc: 53.26%\n",
      "Epoch 1/3, Batch 730, Loss: 1.5278, Acc: 53.21%\n",
      "Epoch 1/3, Batch 740, Loss: 0.4618, Acc: 53.31%\n",
      "Epoch 1/3, Batch 750, Loss: 0.9917, Acc: 53.26%\n",
      "Epoch 1/3, Batch 760, Loss: 0.7201, Acc: 53.22%\n",
      "Epoch 1/3, Batch 770, Loss: 0.3418, Acc: 53.18%\n",
      "Epoch 1/3, Batch 780, Loss: 2.1941, Acc: 53.52%\n",
      "Epoch 1/3, Batch 790, Loss: 0.4778, Acc: 53.60%\n",
      "Epoch 1/3, Batch 800, Loss: 2.5453, Acc: 53.31%\n",
      "Epoch 1/3, Batch 810, Loss: 0.5750, Acc: 53.51%\n",
      "Epoch 1/3, Batch 820, Loss: 0.1975, Acc: 53.71%\n",
      "Epoch 1/3, Batch 830, Loss: 0.9621, Acc: 53.79%\n",
      "Epoch 1/3, Batch 840, Loss: 1.0545, Acc: 53.75%\n",
      "Epoch 1/3, Batch 850, Loss: 0.2004, Acc: 54.05%\n",
      "Epoch 1/3, Batch 860, Loss: 1.5923, Acc: 54.24%\n",
      "Epoch 1/3, Batch 870, Loss: 2.3021, Acc: 54.31%\n",
      "Epoch 1/3, Batch 880, Loss: 0.1110, Acc: 54.48%\n",
      "Epoch 1/3, Batch 890, Loss: 0.7173, Acc: 54.66%\n",
      "Epoch 1/3, Batch 900, Loss: 1.5735, Acc: 54.16%\n",
      "Epoch 1/3, Batch 910, Loss: 1.5229, Acc: 54.12%\n",
      "Epoch 1/3, Batch 920, Loss: 0.5046, Acc: 54.07%\n",
      "Epoch 1/3, Batch 930, Loss: 0.0055, Acc: 54.14%\n",
      "Epoch 1/3, Batch 940, Loss: 2.1850, Acc: 53.88%\n",
      "Epoch 1/3, Batch 950, Loss: 0.2733, Acc: 53.84%\n",
      "Epoch 1/3, Batch 960, Loss: 0.3222, Acc: 53.90%\n",
      "Epoch 1/3, Batch 970, Loss: 0.7640, Acc: 53.96%\n",
      "Epoch 1/3, Batch 980, Loss: 0.7271, Acc: 53.92%\n",
      "Epoch 1/3, Batch 990, Loss: 0.6408, Acc: 53.88%\n",
      "Epoch 1/3, Batch 1000, Loss: 0.1811, Acc: 54.15%\n",
      "Epoch 1/3, Batch 1010, Loss: 0.7931, Acc: 53.91%\n",
      "Epoch 1/3, Batch 1020, Loss: 0.8095, Acc: 53.77%\n",
      "Epoch 1/3, Batch 1030, Loss: 1.1602, Acc: 53.73%\n",
      "Epoch 1/3, Batch 1040, Loss: 0.1649, Acc: 53.70%\n",
      "Epoch 1/3, Batch 1050, Loss: 2.0799, Acc: 53.66%\n",
      "Epoch 1/3, Batch 1060, Loss: 3.4105, Acc: 53.35%\n",
      "Epoch 1/3, Batch 1070, Loss: 0.2416, Acc: 52.94%\n",
      "Epoch 1/3, Batch 1080, Loss: 0.8249, Acc: 52.91%\n",
      "Epoch 1/3, Batch 1090, Loss: 0.2567, Acc: 53.07%\n",
      "Epoch 1/3, Batch 1100, Loss: 0.3542, Acc: 53.22%\n",
      "Epoch 1/3, Batch 1110, Loss: 0.6440, Acc: 53.11%\n",
      "Epoch 1/3, Batch 1120, Loss: 0.5128, Acc: 53.17%\n",
      "Epoch 1/3, Batch 1130, Loss: 0.6678, Acc: 53.05%\n",
      "Epoch 1/3, Batch 1140, Loss: 0.3390, Acc: 53.29%\n",
      "Epoch 1/3, Batch 1150, Loss: 0.4803, Acc: 53.26%\n",
      "Epoch 1/3, Batch 1160, Loss: 0.1869, Acc: 53.40%\n",
      "Epoch 1/3, Batch 1170, Loss: 0.0876, Acc: 53.54%\n",
      "Epoch 1/3, Batch 1180, Loss: 0.0491, Acc: 53.51%\n",
      "Epoch 1/3, Batch 1190, Loss: 2.7441, Acc: 53.65%\n",
      "Epoch 1/3, Batch 1200, Loss: 0.1756, Acc: 53.79%\n",
      "Epoch 1/3, Batch 1210, Loss: 0.4165, Acc: 54.00%\n",
      "Epoch 1/3, Batch 1220, Loss: 0.1870, Acc: 54.05%\n",
      "Epoch 1/3, Batch 1230, Loss: 0.0332, Acc: 54.10%\n",
      "Epoch 1/3, Batch 1240, Loss: 0.0151, Acc: 54.15%\n",
      "Epoch 1/3, Batch 1250, Loss: 0.3983, Acc: 54.12%\n",
      "Epoch 1/3, Batch 1260, Loss: 0.0121, Acc: 54.00%\n",
      "Epoch 1/3, Batch 1270, Loss: 0.0714, Acc: 54.21%\n",
      "Epoch 1/3, Batch 1280, Loss: 0.4201, Acc: 54.33%\n",
      "Epoch 1/3, Batch 1290, Loss: 0.1326, Acc: 54.30%\n",
      "Epoch 1/3, Batch 1300, Loss: 0.2311, Acc: 54.50%\n",
      "Epoch 1/3, Batch 1310, Loss: 0.8739, Acc: 54.39%\n",
      "Epoch 1/3, Batch 1320, Loss: 0.4498, Acc: 54.58%\n",
      "Epoch 1/3, Batch 1330, Loss: 0.0537, Acc: 54.47%\n",
      "Epoch 1/3, Batch 1340, Loss: 0.4373, Acc: 54.74%\n",
      "Epoch 1/3, Batch 1350, Loss: 2.2968, Acc: 54.63%\n",
      "Epoch 1/3, Batch 1360, Loss: 0.6282, Acc: 54.45%\n",
      "Epoch 1/3, Batch 1370, Loss: 1.9035, Acc: 54.41%\n",
      "Epoch 1/3, Batch 1380, Loss: 0.4654, Acc: 54.31%\n",
      "Epoch 1/3, Batch 1390, Loss: 1.9614, Acc: 54.42%\n",
      "Epoch 1/3, Batch 1400, Loss: 2.0109, Acc: 54.39%\n",
      "Epoch 1/3, Batch 1410, Loss: 1.5856, Acc: 54.22%\n",
      "Epoch 1/3, Batch 1420, Loss: 3.2373, Acc: 54.19%\n",
      "Epoch 1/3, Batch 1430, Loss: 1.4701, Acc: 54.16%\n",
      "Epoch 1/3, Batch 1440, Loss: 1.3940, Acc: 54.20%\n",
      "Epoch 1/3, Batch 1450, Loss: 0.2700, Acc: 54.45%\n",
      "Epoch 1/3, Batch 1460, Loss: 0.2662, Acc: 54.62%\n",
      "Epoch 1/3, Batch 1470, Loss: 0.6618, Acc: 54.66%\n",
      "Epoch 1/3, Batch 1480, Loss: 0.5203, Acc: 54.69%\n",
      "Epoch 1/3, Batch 1490, Loss: 0.3306, Acc: 54.73%\n",
      "Epoch 1/3, Batch 1500, Loss: 0.9011, Acc: 54.83%\n",
      "Epoch 1/3, Batch 1510, Loss: 0.2100, Acc: 55.00%\n",
      "Epoch 1/3, Batch 1520, Loss: 0.4468, Acc: 55.16%\n",
      "Epoch 1/3, Batch 1530, Loss: 0.1053, Acc: 55.19%\n",
      "Epoch 1/3, Batch 1540, Loss: 0.2578, Acc: 55.22%\n",
      "Epoch 1/3, Batch 1550, Loss: 1.6685, Acc: 55.25%\n",
      "Epoch 1/3, Batch 1560, Loss: 1.4872, Acc: 55.22%\n",
      "Epoch 1/3, Batch 1570, Loss: 0.0653, Acc: 55.25%\n",
      "Epoch 1/3, Batch 1580, Loss: 0.1021, Acc: 55.22%\n",
      "Epoch 1/3, Batch 1590, Loss: 0.1934, Acc: 55.31%\n",
      "Epoch 1/3, Batch 1600, Loss: 2.4060, Acc: 55.22%\n",
      "Epoch 1/3, Batch 1610, Loss: 0.1931, Acc: 55.18%\n",
      "Epoch 1/3, Batch 1620, Loss: 0.7299, Acc: 55.34%\n",
      "Epoch 1/3, Batch 1630, Loss: 1.8980, Acc: 55.24%\n",
      "Epoch 1/3, Batch 1640, Loss: 0.6235, Acc: 55.15%\n",
      "Epoch 1/3, Batch 1650, Loss: 0.1507, Acc: 55.24%\n",
      "Epoch 1/3, Batch 1660, Loss: 1.3743, Acc: 55.21%\n",
      "Epoch 1/3, Batch 1670, Loss: 1.6753, Acc: 55.30%\n",
      "Epoch 1/3, Batch 1680, Loss: 1.6768, Acc: 55.26%\n",
      "Epoch 1/3, Batch 1690, Loss: 0.9114, Acc: 55.12%\n",
      "Epoch 1/3, Batch 1700, Loss: 1.2231, Acc: 55.14%\n",
      "Epoch 1/3, Batch 1710, Loss: 1.9426, Acc: 55.06%\n",
      "Epoch 1/3, Batch 1720, Loss: 0.4870, Acc: 55.08%\n",
      "Epoch 1/3, Batch 1730, Loss: 0.0925, Acc: 55.17%\n",
      "Epoch 1/3, Batch 1740, Loss: 0.3979, Acc: 55.20%\n",
      "Epoch 1/3, Batch 1750, Loss: 1.7751, Acc: 55.34%\n"
     ]
    }
   ],
   "source": [
    "# Train the model first\n",
    "print(\"Training model...\")\n",
    "trained_model = train_enn(enn, trainloader, epochs, lr, λ, z_dim, device, tb_writer)\n",
    "\n",
    "# Then evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "accuracy, uncertainty = evaluate_enn(trained_model, testloader, z_dim, device)\n",
    "\n",
    "# Save the model\n",
    "torch.save(trained_model.state_dict(), \"enn_halueval_model.pt\")\n",
    "print(\"Model saved to enn_halueval_model.pt\")\n",
    "tb_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m testset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform_test)\n\u001b[1;32m     35\u001b[0m testloader \u001b[38;5;241m=\u001b[39m DataLoader(testset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrain_enn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mλ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mtrain_enn\u001b[0;34m(model, dataloader, epochs, lr, λ, z_dim, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m z \u001b[38;5;241m=\u001b[39m sample_z(x_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), z_dim, device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, y_batch)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# + λ ||θ||² optional weight decay handled by optimizer\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m, in \u001b[0;36mEpistemicNN.forward\u001b[0;34m(self, x, z)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, z):\n\u001b[0;32m---> 10\u001b[0m     μ, φ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m                     \u001b[38;5;66;03m# base logits & features\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     δ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepinet(φ, z)                   \u001b[38;5;66;03m# learnable correction\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     σP \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior(φ, z) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mBaseNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# φζ(x)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(features)               \u001b[38;5;66;03m# μζ(x)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits, features\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torchvision/models/resnet.py:146\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    144\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m--> 146\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m    148\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test with custom examples\n",
    "def predict_with_uncertainty(model, texts, z_dim=16, num_samples=10):\n",
    "    \"\"\"Make predictions with uncertainty estimation\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            # Sample z for each input\n",
    "            z = sample_z(len(texts), z_dim, device)\n",
    "            \n",
    "            # Get model predictions\n",
    "            logits = model(texts, z)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            all_probs.append(probs)\n",
    "    \n",
    "    # Stack all predictions [num_samples, batch_size, num_classes]\n",
    "    stacked_probs = torch.stack(all_probs)\n",
    "    \n",
    "    # Calculate mean and variance\n",
    "    mean_probs = stacked_probs.mean(dim=0)\n",
    "    uncertainty = stacked_probs.var(dim=0).sum(dim=1)\n",
    "    \n",
    "    # Get class predictions\n",
    "    predicted_class = mean_probs.argmax(dim=1)\n",
    "    \n",
    "    # Convert to numpy for easier handling\n",
    "    predicted_class = predicted_class.cpu().numpy()\n",
    "    uncertainty = uncertainty.cpu().numpy()\n",
    "    hallucination_prob = mean_probs[:, 1].cpu().numpy()  # Assuming class 1 is hallucination\n",
    "    \n",
    "    return predicted_class, uncertainty, hallucination_prob\n",
    "\n",
    "# Example inputs - add your own examples here\n",
    "sample_texts = [\n",
    "    \"Q: What is the capital of France? A: Paris is the capital of France.\",\n",
    "    \"Q: How many planets are in our solar system? A: There are 9 planets in our solar system.\",\n",
    "    \"Q: Who wrote 'The Great Gatsby'? A: F. Scott Fitzgerald wrote 'The Great Gatsby'.\",\n",
    "    \"Q: What is the boiling point of water? A: Water boils at 130 degrees Celsius at sea level.\",\n",
    "    \"Q: What's the smallest prime number? A: The smallest prime number is 2.\",\n",
    "    \"Q: What is the tallest mountain? A: Mount Kilimanjaro is the tallest mountain in the world.\",\n",
    "]\n",
    "\n",
    "# Make predictions with uncertainty\n",
    "classes, uncertainties, hallucination_probs = predict_with_uncertainty(trained_model, sample_texts)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n===== HALLUCINATION DETECTION RESULTS =====\")\n",
    "print(\"0 = Factual, 1 = Hallucination\\n\")\n",
    "\n",
    "for i, (text, cls, uncertainty, prob) in enumerate(zip(sample_texts, classes, uncertainties, hallucination_probs)):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {'Hallucination' if cls == 1 else 'Factual'}\")\n",
    "    print(f\"Hallucination Probability: {prob:.4f}\")\n",
    "    print(f\"Epistemic Uncertainty: {uncertainty:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Total examples: {len(sample_texts)}\")\n",
    "print(f\"Detected hallucinations: {sum(classes)}\")\n",
    "print(f\"Average uncertainty: {uncertainties.mean():.4f}\")\n",
    "\n",
    "# Examples of highest and lowest uncertainty\n",
    "most_uncertain_idx = uncertainties.argmax()\n",
    "least_uncertain_idx = uncertainties.argmin()\n",
    "print(f\"\\nMost uncertain: '{sample_texts[most_uncertain_idx]}'\")\n",
    "print(f\"Least uncertain: '{sample_texts[least_uncertain_idx]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
