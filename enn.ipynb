{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nysWrO27V5xY",
        "outputId": "be167661-deec-440f-903e-852e6092d715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting torch-tb-profiler\n",
            "  Downloading torch_tb_profiler-0.4.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.1)\n",
            "Collecting anthropic\n",
            "  Downloading anthropic-0.51.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.169.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m797.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/211.5 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision transformers tqdm requests datasets accelerate bitsandbytes tensorboard torch-tb-profiler openai anthropic google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWtaHtjuV5xZ"
      },
      "outputs": [],
      "source": [
        "# --- Safe flags for Apple-silicon ---\n",
        "import os, platform\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"]           = \"false\"   # avoid fork-after-tokenizer bug\n",
        "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.9\"     # leave 10 % headroom, prevents sudden kills\n",
        "os.environ[\"FLASH_ATTENTION_FORCE_DISABLE\"]    = \"1\"       # disable Flash-Attn v2 path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CkvCm5pV5xZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import platform\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer, logging\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "tb_writer = SummaryWriter(\"runs/halueval_llama\")\n",
        "\n",
        "# Reduce verbosity of transformers\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# Check if GPU is available\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\") # change this to foundry gpu if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZHNJuErV5xa"
      },
      "outputs": [],
      "source": [
        "import torch, os, platform, psutil, time\n",
        "print(\"Torch:\", torch.__version__, \"  Free RAM:\", psutil.virtual_memory().available/1e9, \"GB\")\n",
        "print(\"MPS cap:\", torch.backends.mps.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8dNJmVEV5xa"
      },
      "outputs": [],
      "source": [
        "print(platform.platform(), torch.__version__)\n",
        "print(\"MPS available:\", torch.backends.mps.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXnWL54jV5xa"
      },
      "outputs": [],
      "source": [
        "# Define the base model using Llama from Hugging Face\n",
        "class LlamaBaseNet(nn.Module):\n",
        "    def __init__(self, model_name=\"meta-llama/Llama-2-7b-hf\", num_classes=2):\n",
        "        super().__init__()\n",
        "        # Load Llama model and tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "        # self.backbone = self.backbone.half()           # fp16\n",
        "        self.backbone.gradient_checkpointing_enable()  # save RAM\n",
        "\n",
        "        # If the tokenizer doesn't have a padding token, set it\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Get hidden size from config\n",
        "        self.hidden_size = self.backbone.config.hidden_size\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(self.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, texts):\n",
        "        # Tokenize and move to device\n",
        "        if isinstance(texts, torch.Tensor):\n",
        "            # If input is already tokenized\n",
        "            inputs = {'input_ids': texts}\n",
        "        else:\n",
        "            # If input is raw text\n",
        "            inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "        inputs = {k: v.to(self.classifier.weight.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Get model outputs\n",
        "        with torch.no_grad():  # Don't compute gradients for the backbone\n",
        "            outputs = self.backbone(**inputs)\n",
        "\n",
        "        # Use the last hidden state of the last token for classification\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "        sequence_lengths = torch.ne(inputs['input_ids'], self.tokenizer.pad_token_id).sum(-1) - 1\n",
        "        batch_size = last_hidden_states.shape[0]\n",
        "\n",
        "        # Get the hidden state for the last token in each sequence\n",
        "        features = last_hidden_states[torch.arange(batch_size), sequence_lengths]\n",
        "\n",
        "        # Apply classifier\n",
        "        logits = self.classifier(features)\n",
        "\n",
        "        return logits, features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDTdVZ1oV5xa"
      },
      "outputs": [],
      "source": [
        "# hugging face auth\n",
        "\n",
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "hf_token = os.getenv(\"HUGGING_FACE_KEY\")\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-NYoERjV5xa"
      },
      "outputs": [],
      "source": [
        "# Load HaluEval dataset from Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "def prepare_halueval_data_from_hf():\n",
        "    \"\"\"Load HaluEval dataset from Hugging Face\"\"\"\n",
        "    print(\"Loading HaluEval dataset from Hugging Face...\")\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = \"data/halueval\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Process each split\n",
        "    categories = [\"qa\", \"dialogue\", \"summarization\", \"general\"]\n",
        "\n",
        "    # Prepare train and test sets\n",
        "    for category in categories:\n",
        "        print(f\"Loading {category} dataset...\")\n",
        "        # Load the dataset for this category\n",
        "        dataset = load_dataset(\"pminervini/HaluEval\", category)\n",
        "\n",
        "        # The dataset has a 'data' split containing all examples\n",
        "        data = dataset['data']\n",
        "\n",
        "        # Split into train/test (80/20 split)\n",
        "        splits = data.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "        # Save as jsonl\n",
        "        with open(f\"{output_dir}/{category}_train.jsonl\", 'w', encoding='utf-8') as f:\n",
        "            for item in splits['train']:\n",
        "                formatted_item = {\n",
        "                    'question': item.get('instruction', ''),\n",
        "                    'response': item.get('output', ''),\n",
        "                    'is_hallucination': 1 if item.get('label') == 'hallucinated' else 0\n",
        "                }\n",
        "                f.write(json.dumps(formatted_item) + '\\n')\n",
        "\n",
        "        with open(f\"{output_dir}/{category}_test.jsonl\", 'w', encoding='utf-8') as f:\n",
        "            for item in splits['test']:\n",
        "                formatted_item = {\n",
        "                    'question': item.get('instruction', ''),\n",
        "                    'response': item.get('output', ''),\n",
        "                    'is_hallucination': 1 if item.get('label') == 'hallucinated' else 0\n",
        "                }\n",
        "                f.write(json.dumps(formatted_item) + '\\n')\n",
        "\n",
        "    # Merge all training data\n",
        "    print(\"Merging all training data...\")\n",
        "    with open(f\"{output_dir}/train.jsonl\", 'w', encoding='utf-8') as outfile:\n",
        "        for category in categories:\n",
        "            with open(f\"{output_dir}/{category}_train.jsonl\", 'r', encoding='utf-8') as infile:\n",
        "                outfile.write(infile.read())\n",
        "\n",
        "    # Merge all test data\n",
        "    print(\"Merging all test data...\")\n",
        "    with open(f\"{output_dir}/test.jsonl\", 'w', encoding='utf-8') as outfile:\n",
        "        for category in categories:\n",
        "            with open(f\"{output_dir}/{category}_test.jsonl\", 'r', encoding='utf-8') as infile:\n",
        "                outfile.write(infile.read())\n",
        "\n",
        "    print(\"HaluEval dataset preparation complete!\")\n",
        "    print(f\"Train data: {output_dir}/train.jsonl\")\n",
        "    print(f\"Test data: {output_dir}/test.jsonl\")\n",
        "\n",
        "    return f\"{output_dir}/train.jsonl\", f\"{output_dir}/test.jsonl\"\n",
        "\n",
        "# Run the function to get the paths\n",
        "train_data_path, test_data_path = prepare_halueval_data_from_hf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogU2YmtOV5xa"
      },
      "outputs": [],
      "source": [
        "# 3. Define the Epinet\n",
        "class EpiNet(nn.Module):\n",
        "    def __init__(self, feature_dim, z_dim, hidden_dims, num_classes):\n",
        "        super().__init__()\n",
        "        dims = [feature_dim + z_dim] + hidden_dims + [num_classes]\n",
        "        layers = []\n",
        "        for in_d, out_d in zip(dims, dims[1:]):\n",
        "            layers += [nn.Linear(in_d, out_d), nn.ReLU()]\n",
        "        self.mlp = nn.Sequential(*layers[:-1])  # drop final ReLU\n",
        "\n",
        "    def forward(self, features, z):\n",
        "        # stop-gradient on features\n",
        "        features = features.detach()\n",
        "        x = torch.cat([features, z], dim=1)\n",
        "        return self.mlp(x)\n",
        "\n",
        "# 4. Define the PriorNet\n",
        "class PriorNet(nn.Module):\n",
        "    def __init__(self, feature_dim, z_dim, num_classes):\n",
        "        super().__init__()\n",
        "        # Fixed random weights\n",
        "        self.fc = nn.Linear(feature_dim + z_dim, num_classes)\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = False  # fix weights\n",
        "\n",
        "    def forward(self, features, z):\n",
        "        features = features.detach()\n",
        "        x = torch.cat([features, z], dim=1)\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyfvwapFV5xb"
      },
      "outputs": [],
      "source": [
        "# 5. Wrap into an Epistemic Neural Network\n",
        "class EpistemicNN(nn.Module):\n",
        "    def __init__(self, base: LlamaBaseNet, epinet: EpiNet, prior: PriorNet=None):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        self.epinet = epinet\n",
        "        self.prior = prior\n",
        "\n",
        "    def forward(self, x, z):\n",
        "        logits, features = self.base(x)         # base logits & features\n",
        "        δ = self.epinet(features, z)            # learnable correction\n",
        "        σP = self.prior(features, z) if self.prior else 0\n",
        "        return logits + δ + σP\n",
        "\n",
        "# 6. Sampling epistemic index z\n",
        "def sample_z(batch_size, z_dim, device):\n",
        "    # Gaussian prior\n",
        "    return torch.randn(batch_size, z_dim, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfqmXgG8V5xb"
      },
      "outputs": [],
      "source": [
        "def train_enn(model, dataloader, epochs, lr, λ, z_dim, device, writer):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=λ)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (x_batch, y_batch) in enumerate(dataloader):\n",
        "            # Move tensors to device\n",
        "            x_batch = x_batch.to(device, non_blocking=False)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            # Sample epistemic indices\n",
        "            z = sample_z(len(y_batch), z_dim, device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(x_batch, z)\n",
        "            loss = F.cross_entropy(logits, y_batch)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track accuracy\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = logits.max(1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += predicted.eq(y_batch).sum().item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}, \"\n",
        "                      f\"Acc: {100.*correct/total:.2f}%\")\n",
        "\n",
        "            avg_loss = total_loss / (batch_idx + 1)\n",
        "            acc      = 100. * correct / total\n",
        "            global_step = epoch * len(dataloader) + batch_idx\n",
        "            writer.add_scalar(\"train/loss\", loss.item(), global_step)\n",
        "            writer.add_scalar(\"train/acc\",  acc,      global_step)\n",
        "\n",
        "            if global_step % 50 == 0:\n",
        "                writer.flush()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {total_loss/(batch_idx+1):.4f}, \"\n",
        "              f\"Accuracy: {100.*correct/total:.2f}%\")\n",
        "\n",
        "        torch.mps.empty_cache()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Also update the evaluation function\n",
        "def evaluate_enn(model, dataloader, z_dim, device, num_samples=10):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    epistemic_uncertainty = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in dataloader:\n",
        "            x_batch = x_batch.to(device, non_blocking=False)\n",
        "            y_batch = y_batch.to(device)\n",
        "            batch_size = len(x_batch)\n",
        "\n",
        "            # Sample multiple z for each input\n",
        "            all_logits = []\n",
        "            for _ in range(num_samples):\n",
        "                z = sample_z(batch_size, z_dim, device)\n",
        "                logits = model(x_batch, z)\n",
        "                all_logits.append(logits)\n",
        "\n",
        "            # Stack all predictions\n",
        "            stacked_logits = torch.stack(all_logits)  # [num_samples, batch_size, num_classes]\n",
        "\n",
        "            # Mean prediction\n",
        "            mean_logits = stacked_logits.mean(dim=0)\n",
        "            _, predicted = mean_logits.max(1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += predicted.eq(y_batch).sum().item()\n",
        "\n",
        "            # Calculate uncertainty - variance across samples\n",
        "            uncertainty = stacked_logits.var(dim=0).sum(dim=1)  # [batch_size]\n",
        "            epistemic_uncertainty.append(uncertainty)\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    avg_uncertainty = torch.cat(epistemic_uncertainty).mean().item()\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Average Epistemic Uncertainty: {avg_uncertainty:.4f}\")\n",
        "\n",
        "    return accuracy, avg_uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo9dgCFZV5xb"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "num_classes = 2  # Binary classification for hallucination detection\n",
        "z_dim = 16\n",
        "hidden_dims = [128, 64]\n",
        "lr = 1e-5  # Lower learning rate for LLM fine-tuning\n",
        "epochs = 3  # Reduce epochs for faster training with large model\n",
        "λ = 1e-5\n",
        "batch_size = 1  # Smaller batch size for large model\n",
        "max_length = 128\n",
        "import random\n",
        "\n",
        "# Choose a Llama model - use a smaller version if memory is limited\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Start with smaller model for testing\n",
        "\n",
        "class HaluEvalDataset(Dataset):\n",
        "    def __init__(self, data_items, tokenizer):\n",
        "        self.data = data_items\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # ---------- 1. pick the *content* string ----------\n",
        "        if \"question\" in item:          # QA\n",
        "            content = f\"Context: {item['knowledge']}\\nQuestion: {item['question']}\"\n",
        "            gold   = item[\"right_answer\"]\n",
        "            halluc = item[\"hallucinated_answer\"]\n",
        "\n",
        "        elif \"dialogue_history\" in item:      # Dialogue\n",
        "            content = f\"Context: {item['knowledge']}\\nDialogue: {item['dialogue_history']}\"\n",
        "            gold   = item[\"right_response\"]\n",
        "            halluc = item[\"hallucinated_response\"]\n",
        "\n",
        "        elif \"document\" in item:              # Summarisation\n",
        "            content = f\"Document: {item['document']}\"\n",
        "            gold   = item[\"right_summary\"]\n",
        "            halluc = item[\"hallucinated_summary\"]\n",
        "\n",
        "        else:                                 # General split\n",
        "            content = f\"User query: {item['user_query']}\"\n",
        "            # general‐split has only one answer plus a Yes/No label\n",
        "            gold   = item[\"chatgpt_response\"]\n",
        "            halluc = None                     # there is no alt-answer\n",
        "\n",
        "        # ---------- 2. choose gold vs hallucinated version ----------\n",
        "        if halluc is not None and torch.rand(1).item() > 0.5:\n",
        "            text  = f\"{content}\\nAnswer: {halluc}\"\n",
        "            label = 1            # hallucination\n",
        "        else:\n",
        "            text  = f\"{content}\\nAnswer: {gold}\"\n",
        "            label = 0            # factual\n",
        "\n",
        "        # ---------- 3. tokenise ----------\n",
        "        enc = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return enc[\"input_ids\"].squeeze(0), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Initialize models\n",
        "print(\"Initializing models...\")\n",
        "base = LlamaBaseNet(model_name, num_classes).to(device)\n",
        "feature_dim = base.hidden_size\n",
        "epinet = EpiNet(feature_dim, z_dim, hidden_dims, num_classes).to(device)\n",
        "prior = PriorNet(feature_dim, z_dim, num_classes).to(device)\n",
        "enn = EpistemicNN(base, epinet, prior).to(device)\n",
        "\n",
        "# Create train/test splits from the loaded HaluEval dataset\n",
        "print(\"Loading dataset...\")\n",
        "all_data = []\n",
        "categories = [\"qa\", \"dialogue\", \"summarization\", \"general\"]\n",
        "\n",
        "for category in categories:\n",
        "    print(f\"Loading {category} dataset...\")\n",
        "    dataset = load_dataset(\"pminervini/HaluEval\", category)\n",
        "    all_data.extend(dataset['data'])\n",
        "\n",
        "# Shuffle and split the data\n",
        "random.seed(42)\n",
        "random.shuffle(all_data)\n",
        "split_idx = int(0.8 * len(all_data))\n",
        "train_data = all_data[:split_idx]\n",
        "test_data = all_data[split_idx:]\n",
        "\n",
        "# -------------------- build proper Dataset objects --------------------\n",
        "train_dataset = HaluEvalDataset(train_data, base.tokenizer)\n",
        "test_dataset  = HaluEvalDataset(test_data,  base.tokenizer)\n",
        "\n",
        "trainloader = DataLoader(train_dataset,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True)\n",
        "\n",
        "testloader  = DataLoader(test_dataset,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=False)\n",
        "\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Testing samples: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMRVwomnV5xb"
      },
      "outputs": [],
      "source": [
        "# small_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
        "# enn.eval();                # no grads\n",
        "# x,y = next(iter(small_loader))\n",
        "# z   = sample_z(1, z_dim, device)\n",
        "# with torch.no_grad():      # forward only\n",
        "#     _ = enn(x.to(device), z)\n",
        "# print(\"✓ tiny batch ran\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQRo-GTBV5xb"
      },
      "outputs": [],
      "source": [
        "import torch, gc\n",
        "\n",
        "torch.mps.empty_cache()   # releases unused cached blocks\n",
        "gc.collect()              # Python-side garbage collection\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ENN from weights\n",
        "def load_enn_weights(model, weights_path):\n",
        "    \"\"\"Load pre-trained weights into the ENN model\"\"\"\n",
        "    print(f\"Loading weights from {weights_path}...\")\n",
        "    checkpoint = torch.load(weights_path, map_location=device)\n",
        "\n",
        "    # Load base model weights\n",
        "    if 'base' in checkpoint:\n",
        "        model.base.load_state_dict(checkpoint['base'])\n",
        "        print(\"Loaded base model weights\")\n",
        "\n",
        "    # Load epinet weights\n",
        "    if 'epinet' in checkpoint:\n",
        "        model.epinet.load_state_dict(checkpoint['epinet'])\n",
        "        print(\"Loaded epinet weights\")\n",
        "\n",
        "    # Load prior weights (if they exist)\n",
        "    if 'prior' in checkpoint and model.prior is not None:\n",
        "        model.prior.load_state_dict(checkpoint['prior'])\n",
        "        print(\"Loaded prior weights\")\n",
        "\n",
        "    print(\"All weights loaded successfully!\")\n",
        "    return model\n",
        "\n",
        "# Then load the weights\n",
        "weights_path = \"content/\"  # Update this path to your weights file\n",
        "enn = load_enn_weights(enn, weights_path)\n",
        "\n",
        "# Set model to evaluation mode since we're not training\n",
        "enn.eval()"
      ],
      "metadata": {
        "id": "Pv160oo868r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dWRLHGZh7dD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw6iF_oZV5xb"
      },
      "outputs": [],
      "source": [
        "# Train the model first\n",
        "print(\"Training model...\")\n",
        "trained_model = train_enn(enn, trainloader, epochs, lr, λ, z_dim, device, tb_writer)\n",
        "\n",
        "# Then evaluate the model\n",
        "print(\"Evaluating model...\")\n",
        "accuracy, uncertainty = evaluate_enn(trained_model, testloader, z_dim, device)\n",
        "\n",
        "# Save the model\n",
        "torch.save(trained_model.state_dict(), \"enn_halueval_model.pt\")\n",
        "print(\"Model saved to enn_halueval_model.pt\")\n",
        "tb_writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AjYeLPZV5xc"
      },
      "outputs": [],
      "source": [
        "Z_DIM          = 16              # must match training\n",
        "SAMPLES_Z      = 30              # how many z draws per candidate\n",
        "TRADE_OFF_LAMB = 0.5             # higher ⇒ penalise uncertainty more\n",
        "# DEVICE         = \"mps\" if torch.backends.mps.is_available() else (\n",
        "#                  \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE = \"cpu\"\n",
        "\n",
        "base   = LlamaBaseNet(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", num_classes=2).to(DEVICE)\n",
        "epinet = EpiNet(base.hidden_size, Z_DIM, [128, 64], num_classes=2).to(DEVICE)\n",
        "prior  = PriorNet(base.hidden_size, Z_DIM, num_classes=2).to(DEVICE)\n",
        "enn    = EpistemicNN(base, epinet, prior).to(DEVICE)\n",
        "\n",
        "state_dict = torch.load(\"enn_halueval_model.pt\", map_location=DEVICE)\n",
        "enn.load_state_dict(state_dict)\n",
        "enn.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def score_answer(raw_text):\n",
        "    \"\"\"Return (prob_factual, variance) for one candidate string.\"\"\"\n",
        "    logits_all = []\n",
        "    for _ in range(SAMPLES_Z):\n",
        "        z = sample_z(1, Z_DIM, DEVICE)\n",
        "        logits = enn(raw_text, z)          # ← only one tensor comes back\n",
        "        logits_all.append(logits)          # shape [1,2]\n",
        "\n",
        "    logits_stacked = torch.stack(logits_all)      # [S,1,2]\n",
        "    mean_logits    = logits_stacked.mean(0).squeeze()   # [2]\n",
        "    variance       = logits_stacked.var(0).sum().item()\n",
        "    prob_factual   = torch.softmax(mean_logits, dim=-1)[0].item()\n",
        "    return prob_factual, variance\n",
        "\n",
        "def select_best(question, answers, lam=TRADE_OFF_LAMB):\n",
        "    rows = []\n",
        "    for ans in answers:\n",
        "        text = f\"{question}\\nAnswer: {ans}\"           # keep template same as training\n",
        "        p, var = score_answer(text)\n",
        "        score  = p - lam * var\n",
        "        rows.append((score, p, var, ans))\n",
        "    rows.sort(reverse=True)\n",
        "    return rows[0], rows     # (best_row, all_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CjYTIpoV5xc"
      },
      "outputs": [],
      "source": [
        "# MAKE SURE TO BENCHMARK\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "def get_openai_ensemble(question, temps=[0.01, 0.3, 0.7, 1.0], model=\"gpt-4o\"):\n",
        "    answers = []\n",
        "    for temp in temps:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": question}],\n",
        "            temperature=temp,\n",
        "            max_completion_tokens=64\n",
        "        )\n",
        "        answers.append(response.choices[0].message.content)\n",
        "    return answers\n",
        "\n",
        "# Example usage\n",
        "question = \"Who discovered the fibonacci sequence\"\n",
        "ensemble_answers = get_openai_ensemble(question)\n",
        "for i, ans in enumerate(ensemble_answers):\n",
        "    print(f\"Temperature {['0.01','0.3','0.7','1.0'][i]}: {ans}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebj8rHQxV5xc"
      },
      "outputs": [],
      "source": [
        "# currently all the same answer, so nothing crazy\n",
        "\n",
        "best, table = select_best(question, ensemble_answers)\n",
        "print(\"===== ENSEMBLE EVALUATION =====\")\n",
        "print(f\"Best answer (score={best[0]:+.3f}, p_factual={best[1]:.3f}, var={best[2]:.3f}):\")\n",
        "print(best[3])\n",
        "print(\"\\nAll answers ranked by score (score = p_factual - λ·var):\")\n",
        "for rank, (score, p_factual, var, ans) in enumerate(table, 1):\n",
        "    print(f\"\\n{rank}. Temperature {['0.01','0.3','0.7','1.0'][rank-1]}:\")\n",
        "    print(f\"   Score: {score:+.3f} | p_factual: {p_factual:.3f} | var: {var:.3f}\")\n",
        "    print(f\"   Answer: {ans}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXJjI0cSV5xc"
      },
      "outputs": [],
      "source": [
        "# MAKE SURE TO BENCHMARK\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from anthropic import Anthropic\n",
        "import google.genai as genai\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Load API keys and initialize clients\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "print(\"Environment check:\")\n",
        "print(f\"OpenAI API key loaded: {'Yes' if OPENAI_API_KEY else 'No'}\")\n",
        "print(f\"Anthropic API key loaded: {'Yes' if ANTHROPIC_API_KEY else 'No'}\")\n",
        "print(f\"Google API key loaded: {'Yes' if GOOGLE_API_KEY else 'No'}\")\n",
        "\n",
        "# Initialize clients if keys are available\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None\n",
        "anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY) if ANTHROPIC_API_KEY else None\n",
        "google_client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# if GOOGLE_API_KEY:\n",
        "#     genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "def get_multi_provider_ensemble(question, providers=None):\n",
        "    \"\"\"Get answers from multiple providers with different settings.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to ask\n",
        "        providers (list): List of tuples (provider, model, temp)\n",
        "                        If None, uses default configuration\n",
        "\n",
        "    Default providers:\n",
        "    [\n",
        "        (\"openai\", \"gpt-4\", 0.3),\n",
        "        (\"openai\", \"gpt-3.5-turbo\", 0.3),\n",
        "        (\"anthropic\", \"claude-3-opus\", 0.3),\n",
        "        (\"anthropic\", \"claude-3-sonnet\", 0.3),\n",
        "        (\"google\", \"gemini-pro\", 0.3)\n",
        "    ]\n",
        "\n",
        "    Returns:\n",
        "        list: List of answers from different providers\n",
        "        list: List of provider descriptions for each answer\n",
        "    \"\"\"\n",
        "    if providers is None:\n",
        "        providers = [\n",
        "            (\"openai\", \"gpt-4\", 0.3),\n",
        "            (\"openai\", \"gpt-3.5-turbo\", 0.3),\n",
        "            (\"anthropic\", \"claude-3-7-sonnet-20250219\", 0.3),\n",
        "            (\"anthropic\", \"claude-3-5-sonnet-20241022\", 0.3),\n",
        "            (\"google\", \"gemini-2.0-flash\", 0.3)\n",
        "        ]\n",
        "\n",
        "    answers = []\n",
        "    provider_info = []  # Track which provider/model gave which answer\n",
        "\n",
        "    for provider, model, temp in providers:\n",
        "        try:\n",
        "            if provider == \"openai\" and openai_client:\n",
        "                response = openai_client.chat.completions.create(\n",
        "                    model=model,\n",
        "                    messages=[{\"role\": \"user\", \"content\": question}],\n",
        "                    temperature=temp,\n",
        "                    max_tokens=64\n",
        "                )\n",
        "                answers.append(response.choices[0].message.content)\n",
        "                provider_info.append(f\"{provider}-{model} (temp={temp})\")\n",
        "\n",
        "            elif provider == \"anthropic\" and anthropic_client:\n",
        "                response = anthropic_client.messages.create(\n",
        "                    model=model,\n",
        "                    max_tokens=64,\n",
        "                    temperature=temp,\n",
        "                    messages=[{\"role\": \"user\", \"content\": question}]\n",
        "                )\n",
        "                answers.append(response.content[0].text)\n",
        "                provider_info.append(f\"{provider}-{model} (temp={temp})\")\n",
        "\n",
        "            elif provider == \"google\" and GOOGLE_API_KEY:\n",
        "                # model_obj = genai.GenerativeModel(model)\n",
        "                response = google_client.models.generate_content(\n",
        "                    model=model,\n",
        "                    contents=question,\n",
        "                    config=genai.types.GenerateContentConfig(\n",
        "                        temperature=temp,\n",
        "                        max_output_tokens=64\n",
        "                    )\n",
        "                )\n",
        "                answers.append(response.text)\n",
        "                provider_info.append(f\"{provider}-{model} (temp={temp})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {provider} {model}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return answers, provider_info\n",
        "\n",
        "# Example usage\n",
        "question = \"Who discovered the fibonacci sequence\"\n",
        "\n",
        "# Default usage with all providers\n",
        "answers, providers = get_multi_provider_ensemble(question)\n",
        "\n",
        "# Print answers with provider information\n",
        "print(\"\\nEnsemble Responses:\")\n",
        "for ans, provider in zip(answers, providers):\n",
        "    print(f\"\\n{provider}:\")\n",
        "    print(ans)\n",
        "\n",
        "# Custom provider configuration example\n",
        "custom_providers = [\n",
        "    (\"openai\", \"gpt-4o\", 0.1),\n",
        "    (\"openai\", \"gpt-4o\", 0.7),\n",
        "    (\"anthropic\", \"claude-3-7-sonnet-20250219\", 0.3),\n",
        "    (\"google\", \"gemini-2.0-flash\", 0.5)\n",
        "]\n",
        "\n",
        "# Get answers with custom configuration\n",
        "custom_answers, custom_providers = get_multi_provider_ensemble(question, custom_providers)\n",
        "\n",
        "# Evaluate with ENN\n",
        "best, table = select_best(question, custom_answers)\n",
        "print(\"\\n===== ENSEMBLE EVALUATION =====\")\n",
        "print(f\"Best answer (score={best[0]:+.3f}, p_factual={best[1]:.3f}, var={best[2]:.3f}):\")\n",
        "print(best[3])\n",
        "print(\"\\nAll answers ranked by score (score = p_factual - λ·var):\")\n",
        "for rank, (score, p_factual, var, ans) in enumerate(table, 1):\n",
        "    print(f\"\\n{rank}. {custom_providers[rank-1]}:\")\n",
        "    print(f\"   Score: {score:+.3f} | p_factual: {p_factual:.3f} | var: {var:.3f}\")\n",
        "    print(f\"   Answer: {ans}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTx1SFWkV5xc"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from anthropic import Anthropic\n",
        "import google.genai as genai\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize API clients\n",
        "openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
        "anthropic_client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
        "google_client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "def get_multi_model_ensemble(question, knowledge=None):\n",
        "    \"\"\"Get answers from 6 specified models with different configurations.\n",
        "\n",
        "    Models:\n",
        "    1. GPT-4 (o4-mini)\n",
        "    2. GPT-4 (o1-mini)\n",
        "    3. Claude 3 Sonnet\n",
        "    4. Claude 3 Haiku\n",
        "    5. Gemini Flash Lite\n",
        "    6. Gemini Flash Preview\n",
        "    \"\"\"\n",
        "    answers = []\n",
        "    provider_info = []\n",
        "\n",
        "    # Format the prompt with knowledge if provided\n",
        "    prompt = f\"Knowledge: {knowledge}\\nQuestion: {question}\" if knowledge else question\n",
        "\n",
        "    try:\n",
        "        # 1. GPT-4 (o4-mini)\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3,\n",
        "            max_completion_tokens=64\n",
        "        )\n",
        "        answers.append(response.choices[0].message.content)\n",
        "        provider_info.append(\"openai-gpt-4o\")\n",
        "\n",
        "        # 2. GPT-4 (o1-mini)\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4.1\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3,\n",
        "            max_completion_tokens=64\n",
        "        )\n",
        "        answers.append(response.choices[0].message.content)\n",
        "        provider_info.append(\"openai-o1-mini\")\n",
        "\n",
        "        # 3. Claude 3 Sonnet\n",
        "        response = anthropic_client.messages.create(\n",
        "            model=\"claude-3-7-sonnet-20250219\",\n",
        "            max_tokens=64,\n",
        "            temperature=0.3,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        answers.append(response.content[0].text)\n",
        "        provider_info.append(\"anthropic-claude-3-sonnet\")\n",
        "\n",
        "        # 4. Claude 3 Haiku\n",
        "        response = anthropic_client.messages.create(\n",
        "            model=\"claude-3-5-haiku-20241022\",\n",
        "            max_tokens=64,\n",
        "            temperature=0.3,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        answers.append(response.content[0].text)\n",
        "        provider_info.append(\"anthropic-claude-3-haiku\")\n",
        "\n",
        "        # 5. Gemini Flash Lite\n",
        "\n",
        "        # FIX GEMINI CHAT COMPLETION SYNTAX\n",
        "\n",
        "        # model = genai.GenerativeModel(\"gemini-2.0-flash-lite\")\n",
        "        response = google_client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash-lite\",  # or \"gemini-2.5-flash-preview-04-17\"\n",
        "        contents=prompt,\n",
        "        config=genai.types.GenerateContentConfig(\n",
        "            temperature=0.3,\n",
        "            max_output_tokens=64\n",
        "            )\n",
        "        )\n",
        "        answers.append(response.text)\n",
        "        provider_info.append(\"google-gemini-flash-lite\")\n",
        "\n",
        "        # 6. Gemini Flash Preview\n",
        "        # model = genai.GenerativeModel(\"gemini-2.5-flash-preview-04-17\")\n",
        "        response = google_client.models.generate_content(\n",
        "            model=\"gemini-2.5-flash-preview-04-17\",\n",
        "            contents=question,\n",
        "            config=genai.types.GenerateContentConfig(\n",
        "                temperature=0.3,\n",
        "                max_output_tokens=64\n",
        "            )\n",
        "        )\n",
        "        answers.append(response.text)\n",
        "        provider_info.append(\"google-gemini-flash-preview\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during API calls: {str(e)}\")\n",
        "\n",
        "    return answers, provider_info\n",
        "\n",
        "# Test the ensemble with HaluEval dataset\n",
        "def evaluate_on_halueval(num_samples=5):\n",
        "    \"\"\"Evaluate the ensemble on HaluEval dataset samples.\"\"\"\n",
        "    # Load HaluEval dataset\n",
        "    from datasets import load_dataset\n",
        "\n",
        "    # Load a few samples from each category\n",
        "    results = []\n",
        "    categories = [\"qa\", \"dialogue\", \"summarization\", \"general\"]\n",
        "\n",
        "    for category in categories:\n",
        "        dataset = load_dataset(\"pminervini/HaluEval\", category, split=\"data\")\n",
        "        samples = dataset.select(range(min(num_samples, len(dataset))))\n",
        "\n",
        "        for item in samples:\n",
        "            # Get the question and knowledge\n",
        "            if \"question\" in item:\n",
        "                question = item[\"question\"]\n",
        "                knowledge = item.get(\"knowledge\", \"\")\n",
        "            elif \"dialogue_history\" in item:\n",
        "                question = item[\"dialogue_history\"]\n",
        "                knowledge = item.get(\"knowledge\", \"\")\n",
        "            elif \"document\" in item:\n",
        "                question = \"Summarize this document\"\n",
        "                knowledge = item[\"document\"]\n",
        "            else:\n",
        "                question = item[\"user_query\"]\n",
        "                knowledge = \"\"\n",
        "\n",
        "            # Get ensemble answers\n",
        "            answers, providers = get_multi_model_ensemble(question, knowledge)\n",
        "\n",
        "            # Evaluate with ENN\n",
        "            best, table = select_best(question, answers)\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                \"category\": category,\n",
        "                \"question\": question,\n",
        "                \"knowledge\": knowledge,\n",
        "                \"answers\": answers,\n",
        "                \"providers\": providers,\n",
        "                \"best_answer\": best,\n",
        "                \"all_scores\": table\n",
        "            })\n",
        "\n",
        "            # Print results for this sample\n",
        "            print(f\"\\n===== {category.upper()} SAMPLE =====\")\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Knowledge: {knowledge[:100]}...\")\n",
        "            print(\"\\nModel responses and scores:\")\n",
        "            for (score, p_factual, var, ans), provider in zip(table, providers):\n",
        "                print(f\"\\n{provider}:\")\n",
        "                print(f\"Score: {score:+.3f} | p_factual: {p_factual:.3f} | var: {var:.3f}\")\n",
        "                print(f\"Answer: {ans[:100]}...\")\n",
        "\n",
        "            print(\"\\nBest Answer:\")\n",
        "            print(f\"Score: {best[0]:+.3f} | p_factual: {best[1]:.3f} | var: {best[2]:.3f}\")\n",
        "            print(f\"Answer: {best[3]}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run evaluation\n",
        "print(\"Starting HaluEval evaluation...\")\n",
        "results = evaluate_on_halueval(num_samples=2)  # Test with k samples per category\n",
        "print(\"\\nEvaluation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw4oS_ZOV5xc"
      },
      "outputs": [],
      "source": [
        "# Import required libraries for evaluation (with fixed tqdm)\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from anthropic import Anthropic\n",
        "import google.genai as genai\n",
        "import time\n",
        "import json\n",
        "from tqdm import tqdm  # Using regular tqdm instead of tqdm.notebook\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Configure PyTorch to use MPS (Metal Performance Shaders) on Mac for faster execution\n",
        "def setup_device():\n",
        "    \"\"\"Set up the most efficient device available (MPS on Mac, CUDA on PC, or CPU)\"\"\"\n",
        "    if torch.backends.mps.is_available():\n",
        "        # Set MPS memory management for MacOS\n",
        "        os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.7\"  # Default is 0.9\n",
        "        os.environ[\"PYTORCH_MPS_LOW_WATERMARK_RATIO\"] = \"0.5\"   # Default is 0.7\n",
        "        device = torch.device(\"mps\")\n",
        "        print(\"Using MPS (Metal Performance Shaders) device\")\n",
        "    elif torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(\"Using CUDA device\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"Using CPU device\")\n",
        "    return device\n",
        "\n",
        "# Set device for ENN model\n",
        "device = setup_device()\n",
        "\n",
        "# Initialize API clients\n",
        "openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
        "anthropic_client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
        "google_client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))\n",
        "\n",
        "def get_multi_model_ensemble(question, knowledge=None):\n",
        "    \"\"\"Get answers from 6 specified models with different configurations.\"\"\"\n",
        "    answers = []\n",
        "    provider_info = []\n",
        "\n",
        "    # Format the prompt with knowledge if provided\n",
        "    prompt = f\"Knowledge: {knowledge}\\nQuestion: {question}\" if knowledge else question\n",
        "\n",
        "    try:\n",
        "        # 1. GPT-4 (o4-mini)\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3,\n",
        "            max_completion_tokens=64\n",
        "        )\n",
        "        answers.append(response.choices[0].message.content)\n",
        "        provider_info.append(\"openai-gpt-4o\")\n",
        "\n",
        "        # 2. GPT-4 (o1-mini)\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4.1\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3,\n",
        "            max_completion_tokens=64\n",
        "        )\n",
        "        answers.append(response.choices[0].message.content)\n",
        "        provider_info.append(\"openai-o1-mini\")\n",
        "\n",
        "        # 3. Claude 3 Sonnet\n",
        "        response = anthropic_client.messages.create(\n",
        "            model=\"claude-3-7-sonnet-20250219\",\n",
        "            max_tokens=64,\n",
        "            temperature=0.3,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        answers.append(response.content[0].text)\n",
        "        provider_info.append(\"anthropic-claude-3-sonnet\")\n",
        "\n",
        "        # 4. Claude 3 Haiku\n",
        "        response = anthropic_client.messages.create(\n",
        "            model=\"claude-3-5-haiku-20241022\",\n",
        "            max_tokens=64,\n",
        "            temperature=0.3,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        answers.append(response.content[0].text)\n",
        "        provider_info.append(\"anthropic-claude-3-haiku\")\n",
        "\n",
        "        # 5. Gemini Flash Lite\n",
        "        response = google_client.models.generate_content(\n",
        "            model=\"gemini-2.0-flash-lite\",\n",
        "            contents=prompt,\n",
        "            config=genai.types.GenerateContentConfig(\n",
        "                temperature=0.3,\n",
        "                max_output_tokens=64\n",
        "            )\n",
        "        )\n",
        "        answers.append(response.text)\n",
        "        provider_info.append(\"google-gemini-flash-lite\")\n",
        "\n",
        "        # 6. Gemini Flash Preview\n",
        "        response = google_client.models.generate_content(\n",
        "            model=\"gemini-2.5-flash-preview-04-17\",\n",
        "            contents=prompt,\n",
        "            config=genai.types.GenerateContentConfig(\n",
        "                temperature=0.3,\n",
        "                max_output_tokens=64\n",
        "            )\n",
        "        )\n",
        "        answers.append(response.text)\n",
        "        provider_info.append(\"google-gemini-flash-preview\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during API calls: {str(e)}\")\n",
        "\n",
        "    return answers, provider_info\n",
        "\n",
        "# Function for batched processing of dataset to avoid memory issues\n",
        "def process_in_batches(dataset, batch_size=10):\n",
        "    \"\"\"Process a dataset in batches to avoid memory issues\"\"\"\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        yield dataset.select(range(i, min(i + batch_size, len(dataset))))\n",
        "\n",
        "def evaluate_halueval_fixed(max_samples_per_category=None, save_results=True, checkpoint_interval=10, batch_size=10):\n",
        "    \"\"\"\n",
        "    Evaluate the ensemble on the HaluEval dataset with fixed progress bars.\n",
        "\n",
        "    Args:\n",
        "        max_samples_per_category: Maximum samples to use per category (None for all)\n",
        "        save_results: Whether to save results to disk\n",
        "        checkpoint_interval: Save results every N samples\n",
        "        batch_size: Number of samples to process in each batch\n",
        "    \"\"\"\n",
        "    from datasets import load_dataset\n",
        "\n",
        "    # Initialize results storage\n",
        "    results = {\n",
        "        \"qa\": [],\n",
        "        \"dialogue\": [],\n",
        "        \"summarization\": [],\n",
        "        \"general\": []\n",
        "    }\n",
        "\n",
        "    # Track metrics\n",
        "    metrics = {\n",
        "        \"qa\": {\"scores\": [], \"p_factual\": [], \"variance\": [], \"best_models\": []},\n",
        "        \"dialogue\": {\"scores\": [], \"p_factual\": [], \"variance\": [], \"best_models\": []},\n",
        "        \"summarization\": {\"scores\": [], \"p_factual\": [], \"variance\": [], \"best_models\": []},\n",
        "        \"general\": {\"scores\": [], \"p_factual\": [], \"variance\": [], \"best_models\": []}\n",
        "    }\n",
        "\n",
        "    # Track rate limits and timing\n",
        "    rate_limit_counter = 0\n",
        "    overall_processed = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Process by category\n",
        "    for category in [\"qa\", \"dialogue\", \"summarization\", \"general\"]:\n",
        "        print(f\"\\n===== Processing {category.upper()} category =====\")\n",
        "\n",
        "        # Load dataset\n",
        "        try:\n",
        "            dataset = load_dataset(\"pminervini/HaluEval\", category, split=\"data\")\n",
        "            print(f\"Loaded {len(dataset)} examples from {category} category\")\n",
        "\n",
        "            # Limit samples if specified\n",
        "            if max_samples_per_category is not None:\n",
        "                dataset = dataset.select(range(min(max_samples_per_category, len(dataset))))\n",
        "                print(f\"Using {len(dataset)} examples\")\n",
        "\n",
        "            # Process dataset in batches to manage memory\n",
        "            total_batches = (len(dataset) + batch_size - 1) // batch_size\n",
        "            batch_count = 0\n",
        "\n",
        "            # Process each batch\n",
        "            for batch in tqdm(list(process_in_batches(dataset, batch_size)),\n",
        "                             desc=f\"Processing {category} batches\"):\n",
        "\n",
        "                batch_count += 1\n",
        "                print(f\"Processing batch {batch_count} of {total_batches} for {category}\")\n",
        "\n",
        "                # Process each example in the batch\n",
        "                for idx, item in enumerate(tqdm(list(batch), desc=f\"Batch {batch_count} examples\")):\n",
        "                    global_idx = (batch_count - 1) * batch_size + idx\n",
        "\n",
        "                    # Get the question and knowledge\n",
        "                    if \"question\" in item:\n",
        "                        question = item[\"question\"]\n",
        "                        knowledge = item.get(\"knowledge\", \"\")\n",
        "                    elif \"dialogue_history\" in item:\n",
        "                        question = item[\"dialogue_history\"]\n",
        "                        knowledge = item.get(\"knowledge\", \"\")\n",
        "                    elif \"document\" in item:\n",
        "                        question = \"Summarize this document\"\n",
        "                        knowledge = item[\"document\"]\n",
        "                    else:\n",
        "                        question = item[\"user_query\"]\n",
        "                        knowledge = \"\"\n",
        "\n",
        "                    # Rate limit management (to avoid API throttling)\n",
        "                    if rate_limit_counter >= 20:  # Reset after 20 API calls\n",
        "                        time.sleep(5)  # Wait 5 seconds\n",
        "                        rate_limit_counter = 0\n",
        "\n",
        "                    # Get ensemble answers\n",
        "                    answers, providers = get_multi_model_ensemble(question, knowledge)\n",
        "                    rate_limit_counter += 6  # 6 API calls made\n",
        "\n",
        "                    # Skip if answers list is empty due to errors\n",
        "                    if not answers:\n",
        "                        print(f\"Skipping example {global_idx} due to empty answers\")\n",
        "                        continue\n",
        "\n",
        "                    # Transfer data to MPS device if available\n",
        "                    if torch.backends.mps.is_available():\n",
        "                        # This part depends on how select_best is implemented\n",
        "                        # Make sure to handle device management in select_best\n",
        "                        pass\n",
        "\n",
        "                    # Evaluate with ENN\n",
        "                    try:\n",
        "                        best, table = select_best(question, answers)\n",
        "\n",
        "                        # Record which model had the best answer\n",
        "                        best_model_idx = [score for score, _, _, _ in table].index(best[0])\n",
        "                        best_model = providers[best_model_idx]\n",
        "\n",
        "                        # Store metrics\n",
        "                        metrics[category][\"scores\"].append(best[0])\n",
        "                        metrics[category][\"p_factual\"].append(best[1])\n",
        "                        metrics[category][\"variance\"].append(best[2])\n",
        "                        metrics[category][\"best_models\"].append(best_model)\n",
        "\n",
        "                        # Store full result\n",
        "                        results[category].append({\n",
        "                            \"id\": global_idx,\n",
        "                            \"question\": question,\n",
        "                            \"knowledge\": knowledge,\n",
        "                            \"answers\": answers,\n",
        "                            \"providers\": providers,\n",
        "                            \"best_answer\": best,\n",
        "                            \"all_scores\": table\n",
        "                        })\n",
        "\n",
        "                        overall_processed += 1\n",
        "\n",
        "                        # Print result for this sample\n",
        "                        print(f\"\\n===== {category.upper()} SAMPLE {global_idx} =====\")\n",
        "                        print(f\"Question: {question[:100]}...\")\n",
        "                        print(f\"Knowledge: {knowledge[:100]}...\")\n",
        "                        print(\"\\nBest Answer:\")\n",
        "                        print(f\"Score: {best[0]:+.3f} | p_factual: {best[1]:.3f} | var: {best[2]:.3f}\")\n",
        "                        print(f\"Model: {best_model}\")\n",
        "                        print(f\"Answer: {best[3][:200]}...\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error evaluating example {global_idx}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "                    # Clear MPS cache periodically\n",
        "                    if idx % 5 == 0 and torch.backends.mps.is_available():\n",
        "                        torch.mps.empty_cache()\n",
        "\n",
        "                # Save batch checkpoint\n",
        "                if save_results:\n",
        "                    checkpoint_file = f\"halueval_{category}_batch_{batch_count}.json\"\n",
        "                    with open(checkpoint_file, 'w') as f:\n",
        "                        batch_results = results[category][-len(batch):]\n",
        "                        json.dump(batch_results, f, indent=2)\n",
        "                    print(f\"Saved batch {batch_count} checkpoint for {category}\")\n",
        "\n",
        "                # Clear MPS cache after each batch\n",
        "                if torch.backends.mps.is_available():\n",
        "                    torch.mps.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {category} category: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()  # Print full traceback for debugging\n",
        "\n",
        "        # Save category results\n",
        "        if save_results:\n",
        "            with open(f\"halueval_{category}_results.json\", 'w') as f:\n",
        "                json.dump(results[category], f, indent=2)\n",
        "            print(f\"Saved complete results for {category} category\")\n",
        "\n",
        "    # Calculate and display overall metrics\n",
        "    print(\"\\n===== OVERALL EVALUATION RESULTS =====\")\n",
        "\n",
        "    # Aggregate metrics across all categories\n",
        "    all_scores = []\n",
        "    all_p_factual = []\n",
        "    all_variance = []\n",
        "    all_best_models = []\n",
        "\n",
        "    for category in [\"qa\", \"dialogue\", \"summarization\", \"general\"]:\n",
        "        cat_scores = metrics[category][\"scores\"]\n",
        "        cat_p_factual = metrics[category][\"p_factual\"]\n",
        "        cat_variance = metrics[category][\"variance\"]\n",
        "        cat_best_models = metrics[category][\"best_models\"]\n",
        "\n",
        "        if cat_scores:  # Only process if we have data\n",
        "            # Category stats\n",
        "            avg_score = sum(cat_scores) / len(cat_scores)\n",
        "            avg_p_factual = sum(cat_p_factual) / len(cat_p_factual)\n",
        "            avg_variance = sum(cat_variance) / len(cat_variance)\n",
        "\n",
        "            # Model preference\n",
        "            model_counts = {}\n",
        "            for model in cat_best_models:\n",
        "                model_counts[model] = model_counts.get(model, 0) + 1\n",
        "\n",
        "            preferred_model = max(model_counts.items(), key=lambda x: x[1])\n",
        "\n",
        "            print(f\"\\n{category.upper()} Category Stats:\")\n",
        "            print(f\"  Samples processed: {len(cat_scores)}\")\n",
        "            print(f\"  Average Score: {avg_score:.3f}\")\n",
        "            print(f\"  Average p_factual: {avg_p_factual:.3f}\")\n",
        "            print(f\"  Average Variance: {avg_variance:.3f}\")\n",
        "            print(f\"  Preferred Model: {preferred_model[0]} ({preferred_model[1]} times, {preferred_model[1]/len(cat_best_models)*100:.1f}%)\")\n",
        "\n",
        "            # Extend to global lists\n",
        "            all_scores.extend(cat_scores)\n",
        "            all_p_factual.extend(cat_p_factual)\n",
        "            all_variance.extend(cat_variance)\n",
        "            all_best_models.extend(cat_best_models)\n",
        "\n",
        "    # Global stats\n",
        "    if all_scores:\n",
        "        print(\"\\nGLOBAL Stats:\")\n",
        "        print(f\"  Total samples processed: {len(all_scores)}\")\n",
        "        print(f\"  Average Score: {sum(all_scores)/len(all_scores):.3f}\")\n",
        "        print(f\"  Average p_factual: {sum(all_p_factual)/len(all_p_factual):.3f}\")\n",
        "        print(f\"  Average Variance: {sum(all_variance)/len(all_variance):.3f}\")\n",
        "\n",
        "        # Global model preference\n",
        "        model_counts = {}\n",
        "        for model in all_best_models:\n",
        "            model_counts[model] = model_counts.get(model, 0) + 1\n",
        "\n",
        "        # Sort models by frequency\n",
        "        sorted_models = sorted(model_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(\"\\nModel Performance Ranking:\")\n",
        "        for rank, (model, count) in enumerate(sorted_models, 1):\n",
        "            print(f\"  {rank}. {model}: {count} wins ({count/len(all_best_models)*100:.1f}%)\")\n",
        "\n",
        "    # Calculate elapsed time\n",
        "    elapsed_time = time.time() - start_time\n",
        "    hours, remainder = divmod(elapsed_time, 3600)\n",
        "    minutes, seconds = divmod(remainder, 60)\n",
        "    print(f\"\\nTotal evaluation time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
        "    print(f\"Processing speed: {overall_processed/elapsed_time:.2f} samples per second\")\n",
        "\n",
        "    # Save final aggregated results\n",
        "    if save_results:\n",
        "        with open(\"halueval_full_evaluation_results.json\", 'w') as f:\n",
        "            json.dump({\n",
        "                \"metrics\": metrics,\n",
        "                \"elapsed_time\": elapsed_time,\n",
        "                \"samples_processed\": overall_processed\n",
        "            }, f, indent=2)\n",
        "        print(\"\\nSaved full evaluation results\")\n",
        "\n",
        "    return results, metrics\n",
        "\n",
        "# Run evaluation with the fixed method\n",
        "print(\"Starting HaluEval evaluation with fixed tqdm...\")\n",
        "# Choose how many samples to process per category\n",
        "# For a quick test: max_samples_per_category=5 (20 total samples)\n",
        "# For small test: max_samples_per_category=20 (80 total samples)\n",
        "# For medium test: max_samples_per_category=100 (400 total samples)\n",
        "# For full evaluation: max_samples_per_category=None (all samples)\n",
        "results, metrics = evaluate_halueval_fixed(\n",
        "    max_samples_per_category=20,  # Start with a smaller number to test and then increase over time\n",
        "    batch_size=5,                 # Smaller batch size for testing\n",
        "    checkpoint_interval=5         # Save checkpoints every 5 samples\n",
        ")\n",
        "print(\"\\nEvaluation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYCx6X0eV5xd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}